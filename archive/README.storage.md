# Overview

+ Log (Region, Prefix, SequenceNo)
  + INDEXed on (DataSHA3512)

## Problems being solved

Merkle tree implementations like Trillian provide provable inclusion of log
entries in a way infrastructure cannot invisibly alter. However using
Trillian as a web-scale historical notary requires solving a few additional
problems:

+ Timestamping entries to allow their global ordering to be checked without
  consulting the log, where timestamps are:
  + Verifiably monotonic
  + Verifiably consistent (B=hash(A), B timestamp must be after that of A)
  + Verifiably fair (Alice doesn't get her entries delayed more than Bob)
+ Scaling write throughput to web scale

Timestamping and global ordering are achievable using public block chains at
the cost of write throughput. Throughput can be increased by operating
multiple independent verifiable data structures, at the cost of global
ordering (cannot know the relative order of events written to different
data structures).

We propose the "Merkle weave", an open data structure and service backed by
Spanner and TrueTime, that addresses these problems.

## Design overview

The log is split by region to reduce user-facing latency and split by hash
prefix (to increase write throughput). Each (Region, Prefix) pair defines an
independent Merkle Mountain Range. An MMR is functionally similar to a Merkle
tree, but is sequentially and deterministically built up from the bottom.

The result is a set of MMRs in each region that accept sequential writes
(each write must transactionally consult the most recent previous one).

We can prove the inclusion of any entry within its Region- and
Prefix-specific MMR in the usual way by traversing up form an entry in the
MMR to the MMR's digest.

Multiple independent MMRs introduce flex into the system by potentially
allowing malicious infrastructure to manipulate write order across MMRs. To
guard against this, we interlock the MMRs so that the datastructure
comprising the set of all MMRs is nevertheless lockstep built-up from user
inputs alone.

Interlocking is achieved in two ways. First, at the bottom, leaf entries tie
together different MMRs using deterministic prefixing and hash chaining.
Second, MMRs are tied together when an overall digest is computed across all
MMRs for a region. These techniques are described in the relevant sections
below.

## Appending new entries to the log

DataSHA3512 is first computed using the 64-byte value provided by the user
and the 64-byte salt generated by the server (and returned to the user). The
timestamp is the Spanner commit timestamp (also returned to the user).

To determine the SequenceNo and the NodeSHA3512, we first compute two
prefixes by splitting DataSHA3512 as follows:

```none
  Index:         0 1 2 3 ... == DataSHA3512[0:64]
  Prefix:       |0|1|        == DataSHA3512[0:2]
                |0| |2|
                 | ┌─┘
  CrossPrefix:  |0 2|        == append(DataSHA3512[0:1], DataSHA3512[2:3])
```

Generally, for prefix length N (which must be even):

```go
  Prefix = DataSHA3512[0:N]
  CrossPrefix = append(DataSHA3512[0:N/2], DataSHA3512[N:N+N/2])
```

For a given hash, what is the probability another random hash will have either
the same prefix or a matching crossprefix?

```math
  match = first half AND (second half OR third half)
  p_match = p(first half) * (p(2nd half) + p(third half))
          = 1/(2^(8N)) * (1/(2^(8N)) + 1/(2^(8N)))
          =
```

This ensures that the two prefixes are entirely determined by the DataSHA3512
(which neither the user nor the infrastructure can unilaterally determine),
that CrossPrefix's position relative to Prefix is unpredictable, and yet that
CrossPrefix is "close" to Prefix in the key space (which can be exploited for
server locality when adding entries).

SequenceNo is one larger than the most recent SequenceNo in Prefix.

Based on the SequenceNo, the new entry can be either an MMR leaf or non-leaf.

Non-leaf entries build up the MMR, so their predecessors are determined in
the usual way within Prefix:

```math
Predecessor1 = RightChild(SequenceNo) = entryAt(Prefix, SequenceNo-1)
Predecessor2 = LeftChild(SequenceNo)
```

Leaves don't typically have a predecessor in a Merkle Tree/MMR. We can thus
exploit leaf entries to fix two problems uniquely introduced by this scheme:

  1) ensuring that no two timestamps in any single chain break causality
  2) ensuring that no two timestamps in two different chains break causality

The first predecessor of a leaf is the prior entry in its own prefix chain (the
same as the first predecessor of a non-leaf). The second predecessor of a leaf
is the prior entry in the CrossPrefix chain.

```math
Predecessor1 = entryAt(Prefix, SequenceNo-1)
Predecessor2 = lastEntry(CrossPrefix)
```

In either the leaf or non-leaf case, a Predecessor may not exist (if the
prefix in question has no entries yet). In that case, the predecessor is just
ignored.

NodeSHA3512 for an entry is determined by hashing in order:

  1) (NodeSHA3512, Timestamp) of Predecessor1, if exists
  2) (NodeSHA3512, Timestamp) of Predecessor2, if exists
  3) DataSHA3512

> TODO: leaving out the new entry's own timestamp allows us to write the
> NodeSHA3512 in the same transaction as the entry itself (ensuring they have
> the same timestamp in the database for when we read in the past). But this
> means we don't include all aspects of an entry written at time `t` in the
> NodeSHA3512 written at time `t`.
>
> We can't write NodeSHA3512 to include the new entry's timestamp in the same
> transaction. Thus we can't maintain a useful rolling hash in the entry itself.
>
> We can't write NodeSHA3512 later since then we won't see it when we perform
> a read-in-the-past at the entry's timestamp.
>
> We can't make the entry's timestamp the timestamp of when the NodeSHA3512
> was written because then the timestamp used to compute NodeSHA3512 is
> different from the timestamp thereafter assigned to the record. Querying in
> the past for an entry would always provide the entry before its NodeSHA3512
> was written (is this ok?).
>
> Another alternative is to populate NodeSHA3512 in lock step one-behind. This
> means we may have to consult up to three prefix chains when writing an entry:
> the entry's own, the CrossPrefix (if entry is a leaf), and the CrossPrefix's
> CrossPrefix (if CrossPrefix is a leaf). This would happen 0.5*0.5=25% of the
> time. The third entry (X-prefix of X-prefix) is a direct SequenceNo read
> (since we know the SequenceNo of the X-prefix entry), however this still
> results in a group leader check for the latest value since the read is part of
> a read-write transaction.
>
> The problem is we don't know the timestamp until after the transaction
> fully completes. We can't even do this in borg with direct access to TT since
> the Spanner timestamp would follow the server-chosen TT timestamp and again
> make it unavailable during reads-in-the-past. We'd have to crack open
> Spanner...
>
> Logically, an entry's "moment in time" is only finalized when it is fully
> written, and nothing can be said about the entry until that occurs so
> NodeSHA3512 cannot include that record's timestamp.
>
> Getting OK with this involves considering NodeSHA3512 is never reported
> to the user in relation to an entry, but always in relation to a proof of an
> entry. As a result, the NodeSHA3512 of an entry freezes that entry's _past_,
> not the entry itself. The entry is frozen by NodeSHA3512s of _future_
> entries.

Writing a new leaf requires consulting a different prefix chain which may
reside in a different Spanner group. In a binary MMR, no more than half of
all entries are leaves (MMRs with larger branching factors would have a greater
proportion of entries that are leaves, which would slow down this scheme). The
proximity between Prefix and CrossPrefix means it is likely both can be accessed
in the same Spanner group. Distributed transactions are thus only incurred for
leaves whose prefixes straddle Spanner groups. These slow writes are fairly
distributed by (random) hash prefix, not by user or region. This is possibly the
optimal arrangement given the requirement to interlock MMRs.

## Creating a digest of the full (regional) log

The log logically consists of 2^(8N) prefix chains that each comprise an MMR.

The prefix length N determines how many prefix chains are active in the
region: higher N increases write throughput but also increases the number of
digests that must be reported.

For N=2, assuming two servers/groups are involved in every write and 100 QPS
per Spanner group, we have an upper bound of 65,536/2 * 100 = 3.2M QPS per
region. Additional smaller regions can increase overall throughput.

The digest for a prefix chain can be obtained by consulting the last entry in
that chain, or any earlier entry representing the point in time of interest.

A digest of a prefix chain can be externally reported as:

`<region>:<prefix>:<number of entries in chain>:<last timestamp>:<SHA3512>`

For example:

`NA:ZUU:15673:6546516847616981651689135662165804354685:2wYM0X+pRKcMp1WjriLOcmGQbZtL9Vt0JckBCu86eA9WuD6kHhh0T7+tXWHuTzPVju+oy6DeRoEJDTjqWXxpJQ`

The binary values `<prefix>` and `<SHA3512>` are URL-safe base64-encoded (RFC
4648 §5) with no padding. Timestamp is encoded as nanoseconds from
0001-01-01 00:00:00 UTC.

The digest of a full (regional) log can be computed by gathering digests of
prefix chains at some point in time. This is best done by reading some time
in the past. Spanner ensures we get a consistent view of the entire log
globally when performing this read.

The digest of a regional log can be reported as:

`<region>:<last timestamp>:<SHA3512>`

For example:

`NA:6546516847616981651689135662165804354685:9w/cdNba+DmCdXnIAhhF/c4W2fDTUT1reavvujhzX1hOzfSZ2hyLz4KdlLkzMilXd6Qqh19f613YYNs4hpZ4Tw`

The digest of the global log can be assembed from regional logs and reported
as:

`<last timestamp>:<SHA3512>`

For example:

`6546516847616981651689135662165804354685:Lh7Zv0z6McRkZmmlgCUznzbIastGxKHGDH8qOdOG5w0QIGDsOZmB19SYb53TH56gaEgm481o6acodXTVH0DZNg`

Prefix, regional and global digests are computed and signed by the
infrastructure.

This digest is signed by the infrastructure, and the digest and signature are
hashed and themselves committed to the log. This means that anyone in
posesssion of a prior digest can request a proof that that digest was
included in any future one. The digest, its signature and the DataSHA3512 and
salt of its commitment are published.

> TODO: What if multiple possible digests were created and committed and we
later shift around which one we claim is real? Signature solves that?

We can recreate this digest at any time in the future by doing time-specific
reads in the past at the desired timestamp: Spanner guarantees we will see an
externally consistent view of the database as it was at that time in the
past. Verifiers can do this themselves offline by simply filtering out
entries with timestamps later than the digest timestamp.

Creating digests makes heavy use of caching of the following entries:

+ The NodeSHA3512 and Timestamp of the peaks of a prefix chain
  + older/taller peaks remain among an MMR's peaks for a long time
  + new tall peaks are known by the their SequenceNo, can be pre-emptively
      cached
+ The digest of each prefix chain at a given SequenceNo and Timestamp
+ The digest of the log at a given Timestamp

## Proving inclusion of an entry

Users request proofs of an entry by its DataSHA3512.

> TODO: What is N? How does it evolve? Ok to start with some obscenely high
number? We're just computing predecessors for leaves, trees are otherwise

## Proving inclusion of one digest in a later one

Within a prefix, this is done in the normal way.

> TODO: across all prefixes? across all regions? these are not assembled into a
tree, so compact proofs are not available...

NB: Cannot have a table higher in the hierarchy than Log (e.g. "Prefixes")
since it would limit Log to 4GB per prefix (min. 250k prefixes to hold 1PB).

Write new values spread across prefixes, which requires also quickly looking
up the most recent value to compute the new SequenceNo and recent and prior
values when computing NodeSHA3512.

Memcache:

+ (Region, Prefix, SequenceNo) --> (NodeSHA3512, Timestamp)
+ (Region, Timestamp) --> N:last_timestamp:hash

Lookup by timestamp is done by simply reading in the past. No index needed.

Lookup a DataSHA3512 when responding to user queries.

This index stores Timestamp so that we can access Log via a read in the past.
Making this unique guards against the exceedingly unlikely possibility of
collision.

> TODO: Decision table, actions table (interleave in decision table)

Time-based interval tree

```sql
-- Largest interval containing t.
SELECT Start, End WHERE Start < t AND t >= End
ORDER BY Start DESC, End DESC
LIMIT 1;
```

Deterministically compute predecessors by timestamp.
Fixed 50ms blackout window. Then binary search intervals.

Shard based on prefix of hash of timestamp?

```sql
CREATE TABLE Interval (
    Start           TIMESTAMP NOT NULL,
    End             TIMESTAMP NOT NULL,
    IntervalSHA3513 BYTES(64),
) PRIMARY KEY (Start DESC, End DESC);
```

## Decisions

A decision is a document created with a user-defined identifier (usually a
random GUID) and an expiration window. The decision is logged and timestamped
and a DataSHA3512 from
the log. The decision can be passed around
